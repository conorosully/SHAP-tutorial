{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SHAP to debug a PyTorch Image Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob \n",
    "import random \n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load example image\n",
    "name = \"32_50_c78164b4-40d2-11ed-a47b-a46bb6070c92.jpg\"\n",
    "x = int(name.split(\"_\")[0])\n",
    "y = int(name.split(\"_\")[1])\n",
    "\n",
    "img = Image.open(\"../data/room_1/\" + name)\n",
    "img = np.array(img)\n",
    "cv2.circle(img, (x, y), 8, (0, 255, 0), 3)\n",
    "\n",
    "plt.imshow(img)\n",
    "\n",
    "path = \"/Users/conorosullivan/Google Drive/My Drive/Medium/shap_imagedata/example.png\"\n",
    "plt.savefig(path, bbox_inches='tight',facecolor='w', edgecolor='w', transparent=False,dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths, transform):\n",
    "\n",
    "        self.transform = transform\n",
    "        self.paths = paths\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get image and target (x, y) coordinates\"\"\"\n",
    "\n",
    "        # Read image\n",
    "        path = self.paths[idx]\n",
    "        image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        # Transform image\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # Get target\n",
    "        target = self.get_target(path)\n",
    "        target = torch.Tensor(target)\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "    def get_target(self,path):\n",
    "        \"\"\"Get the target (x, y) coordinates from path\"\"\"\n",
    "\n",
    "        name = os.path.basename(path)\n",
    "        items = name.split('_')\n",
    "        x = items[0]\n",
    "        y = items[1]\n",
    "\n",
    "        # Scale between -1 and 1\n",
    "        x = 2.0 * (int(x)/ 224 - 0.5) # -1 left, +1 right\n",
    "        y = 2.0 * (int(y) / 244 -0.5)# -1 top, +1 bottom\n",
    "\n",
    "        return [x, y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMS = transforms.Compose([\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "all_rooms = False # Change if you want to use all the data\n",
    "\n",
    "paths = glob.glob('../data/room_1/*')\n",
    "if all_rooms:\n",
    "    paths = paths + glob.glob('../data/room_2/*') + glob.glob('../data/room_3/*')\n",
    "\n",
    "# Shuffle the paths\n",
    "random.shuffle(paths)\n",
    "\n",
    "# Create a datasets for training and validation\n",
    "split = int(0.8 * len(paths))\n",
    "train_data = ImageDataset(paths[:split], TRANSFORMS)\n",
    "valid_data = ImageDataset(paths[split:], TRANSFORMS)\n",
    "\n",
    "# Prepare data for Pytorch model\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=valid_data.__len__())\n",
    "\n",
    "print(train_data.__len__())\n",
    "print(valid_data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 2 # x, y\n",
    "device = torch.device('cpu') # or 'cuda' if you have a GPU\n",
    "\n",
    "# RESNET 18\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(512, output_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"direction_model_1\" # Change this to save a new model\n",
    "\n",
    "# Train the model\n",
    "min_loss = np.inf\n",
    "for epoch in range(10):\n",
    "\n",
    "    model = model.train()\n",
    "    for images, target in iter(train_loader):\n",
    "\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Zero gradients of parameters\n",
    "        optimizer.zero_grad()  \n",
    "\n",
    "        # Execute model to get outputs\n",
    "        output = model(images)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = torch.nn.functional.mse_loss(output, target)\n",
    "\n",
    "        # Run backpropogation to accumulate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate validation loss\n",
    "    model = model.eval()\n",
    "\n",
    "    images, target = next(iter(valid_loader))\n",
    "    images = images.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    output = model(images)\n",
    "    valid_loss = torch.nn.functional.mse_loss(output, target)\n",
    "\n",
    "    print(\"Epoch: {}, Validation Loss: {}\".format(epoch, valid_loss.item()))\n",
    "    \n",
    "    if valid_loss < min_loss:\n",
    "        print(\"Saving model\")\n",
    "        torch.save(model, '../models/{}.pth'.format(name))\n",
    "\n",
    "        min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(loaders,labels,save_path = None):\n",
    "\n",
    "    \"\"\"Evaluate direction models with mse and scatter plots\n",
    "        loaders: list of data loaders\n",
    "        labels: list of labels for plot title\"\"\"\n",
    "\n",
    "    n = len(loaders)\n",
    "    fig, axs = plt.subplots(1, n, figsize=(7*n, 6))\n",
    "    fig.patch.set_facecolor('xkcd:white')\n",
    "\n",
    "    # Evalution metrics\n",
    "    for i, loader in enumerate(loaders):\n",
    "\n",
    "        # Load all data\n",
    "        images, target = next(iter(loader))\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output=model(images)\n",
    "\n",
    "        # Get x predictions\n",
    "        x_pred=output.detach().cpu().numpy()[:,0]\n",
    "        x_target=target.cpu().numpy()[:,0]\n",
    "\n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(x_target, x_pred)\n",
    "\n",
    "        # Plot predcitons\n",
    "        axs[i].scatter(x_target,x_pred)\n",
    "        axs[i].plot([-1, 1], \n",
    "                [-1, 1], \n",
    "                color='r', \n",
    "                linestyle='-', \n",
    "                linewidth=2)\n",
    "\n",
    "        axs[i].set_ylabel('Predicted x', size =15)\n",
    "        axs[i].set_xlabel('Actual x', size =15)\n",
    "        axs[i].set_title(\"{0} MSE: {1:.4f}\".format(labels[i], mse),size = 18)\n",
    "\n",
    "    if save_path != None:\n",
    "        fig.savefig(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model \n",
    "model = torch.load('../models/direction_model_1.pth')\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Create new loader for all data\n",
    "train_loader = DataLoader(train_data, batch_size=train_data.__len__())\n",
    "\n",
    "# Evaluate model on training and validation set\n",
    "loaders = [train_loader,valid_loader]\n",
    "labels = [\"Train\",\"Validation\"]\n",
    "\n",
    "path = \"/Users/conorosullivan/Google Drive/My Drive/Medium/shap_imagedata/evaluation_1.png\"\n",
    "model_evaluation(loaders,labels,save_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on data for additonal rooms\n",
    "room_2 = glob.glob('../data/room_2/*')\n",
    "room_3 = glob.glob('../data/room_3/*')\n",
    "\n",
    "room_2_data = ImageDataset(room_2, TRANSFORMS)\n",
    "room_3_data = ImageDataset(room_3, TRANSFORMS)\n",
    "\n",
    "room_2_loader = DataLoader(room_2_data, batch_size=room_2_data.__len__())\n",
    "room_3_loader = DataLoader(room_3_data, batch_size=room_3_data.__len__())\n",
    "\n",
    "# Evaluate model on training and validation set\n",
    "loaders = [room_2_loader ,room_3_loader]\n",
    "labels = [\"Room 2\",\"Room 3\"]\n",
    "\n",
    "path = \"/Users/conorosullivan/Google Drive/My Drive/Medium/shap_imagedata/evaluation_2.png\"\n",
    "model_evaluation(loaders,labels, save_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model \n",
    "model = torch.load('../models/direction_model_2.pth')\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate model on training and validation set\n",
    "loaders = [room_2_loader ,room_3_loader]\n",
    "labels = [\"Room 2\",\"Room 3\"]\n",
    "\n",
    "path = \"/Users/conorosullivan/Google Drive/My Drive/Medium/shap_imagedata/evaluation_3.png\"\n",
    "model_evaluation(loaders,labels,save_path=path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP Explainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model \n",
    "model = torch.load('../models/direction_model_1.pth') #change for different model\n",
    "model.eval()\n",
    "\n",
    "# Use CPU\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 100 images for background\n",
    "shap_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "background, _ = next(iter(shap_loader))\n",
    "background = background.to(device)\n",
    "\n",
    "#Create SHAP explainer \n",
    "explainer = shap.DeepExplainer(model, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test images of right and left turn\n",
    "paths = glob.glob('../data/room_1/*')\n",
    "test_images = [Image.open(paths[0]), Image.open(paths[3])]\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "test_input = [TRANSFORMS(img) for img in test_images]\n",
    "test_input = torch.stack(test_input).to(device)\n",
    "\n",
    "# Get SHAP values\n",
    "shap_values = explainer.shap_values(test_input)\n",
    "\n",
    "# Reshape shap values and images for plotting\n",
    "shap_numpy = list(np.array(shap_values).transpose(0,1,3,4,2))\n",
    "test_numpy = np.array([np.array(img) for img in test_images])\n",
    "\n",
    "shap.image_plot(shap_numpy, test_numpy,show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using gradient explainer\n",
    "explainer = shap.GradientExplainer(model, background)\n",
    "shap_values = explainer.shap_values(test_input)\n",
    "\n",
    "shap_numpy = list(np.array(shap_values).transpose(0,1,3,4,2))\n",
    "\n",
    "shap.image_plot(shap_numpy, test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model trained on room 1, 2 and 3\n",
    "model = torch.load('../models/direction_model_2.pth') #change for different model\n",
    "\n",
    "# Use CPU\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "#Load 100 images for background\n",
    "shap_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "background, _ = next(iter(shap_loader))\n",
    "background = background.to(device)\n",
    "\n",
    "#Create SHAP explainer \n",
    "explainer = shap.DeepExplainer(model, background)\n",
    "\n",
    "# Load test images of right and left turn\n",
    "paths = glob.glob('../data/room_1/*')\n",
    "test_images = [Image.open(paths[0]), Image.open(paths[3])]\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "# Transform images\n",
    "test_input = [TRANSFORMS(img) for img in test_images]\n",
    "test_input = torch.stack(test_input).to(device)\n",
    "\n",
    "# Get SHAP values\n",
    "shap_values = explainer.shap_values(test_input)\n",
    "\n",
    "# Reshape shap values and images for plotting\n",
    "shap_numpy = list(np.array(shap_values).transpose(0,1,3,4,2))\n",
    "test_numpy = np.array([np.array(img) for img in test_images])\n",
    "\n",
    "shap.image_plot(shap_numpy, test_numpy,show=False)\n",
    "plt.savefig(\"/Users/conorosullivan/Google Drive/My Drive/Medium/shap_imagedata/shap_plot_2.png\",facecolor='white',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Gradient Explainer\n",
    "e = shap.GradientExplainer(model, background)\n",
    "shap_values = e.shap_values(test_input)\n",
    "\n",
    "shap_numpy = list(np.array(shap_values).transpose(0,1,3,4,2))\n",
    "test_numpy = np.array([np.array(img) for img in test_images])\n",
    "\n",
    "shap.image_plot(shap_numpy, test_numpy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c0d4fcf1a0a408688084e944cab5ef64e86c1ae9800e884f9b7a2ac0ee51db6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
