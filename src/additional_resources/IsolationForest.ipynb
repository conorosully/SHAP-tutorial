{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Set figure background to white\n",
    "plt.rcParams.update({'figure.facecolor':'white'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset from UCI repository\n",
    "power_consumption = fetch_ucirepo(id=235)\n",
    "\n",
    "print(power_consumption.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all features\n",
    "data = power_consumption.data.features\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# List of features to check\n",
    "feature_columns = ['Global_active_power', 'Global_reactive_power', 'Voltage', \n",
    "                   'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "\n",
    "# Convert feature columns to numeric and replace any errors with NaN\n",
    "data[feature_columns] = data[feature_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows where all feature columns are missing (NaN) \n",
    "data_cleaned = data.dropna(subset=feature_columns, how='all')\n",
    "\n",
    "# Drop rows where ALL feature columns are NaN\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Date' and calculate mean and standard deviation (ignore NaN values)\n",
    "data_aggregated = data_cleaned.groupby('Date')[feature_columns].agg(['mean', 'std'])\n",
    "\n",
    "# Rename columns to the desired format (MEAN_ColumnName, STD_ColumnName)\n",
    "data_aggregated.columns = [\n",
    "    f'{agg_type.upper()}_{col}' for col, agg_type in data_aggregated.columns\n",
    "]\n",
    "\n",
    "# Reset the index\n",
    "data_aggregated.reset_index(inplace=True)\n",
    "\n",
    "# Display the result\n",
    "print(data_aggregated.shape)\n",
    "data_aggregated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_estimators = 100  # Number of trees\n",
    "sample_size = 256  # Number of samples used to train each tree\n",
    "contamination = 0.02  # Expected proportion of anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Features\n",
    "features = data_aggregated.drop('Date', axis=1)\n",
    "\n",
    "# Train Isolation Forest\n",
    "iso_forest = IsolationForest(n_estimators=n_estimators, \n",
    "                             contamination=contamination, \n",
    "                             max_samples=sample_size,\n",
    "                             random_state=42)\n",
    "\n",
    "iso_forest.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aggregated['anomaly_score'] = iso_forest.decision_function(features)\n",
    "data_aggregated['anomaly'] = iso_forest.predict(features)\n",
    "\n",
    "data_aggregated['anomaly'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot normal instances\n",
    "normal = data_aggregated[data_aggregated['anomaly'] == 1]\n",
    "plt.scatter(normal['Date'], normal['anomaly_score'], label='Normal')\n",
    "\n",
    "# Plot anomalies\n",
    "anomalies = data_aggregated[data_aggregated['anomaly'] == -1]\n",
    "plt.scatter(anomalies['Date'], anomalies['anomaly_score'], label='Anomaly')\n",
    "\n",
    "plt.xlabel(\"Instance\")\n",
    "plt.ylabel(\"Anomaly Score\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KernelSHAP with Anomaly Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the anomaly score and TreeSHAP (this code won't work)\n",
    "explainer = shap.TreeExplainer(iso_forest.decision_function, features)\n",
    "shap_values = explainer(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all anomalies and 100 random normal instances\n",
    "normal_sample = np.random.choice(normal.index,size=100,replace=False)\n",
    "sample = np.append(anomalies.index,normal_sample)\n",
    "\n",
    "len(sample) # 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the anomaly score and KernelSHAP\n",
    "explainer = shap.Explainer(iso_forest.decision_function, features)\n",
    "shap_values = explainer(features.iloc[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot waterfall plot of an anomaly\n",
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot waterfall plot of a normal instance\n",
    "shap.plots.waterfall(shap_values[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeanSHAP Plot\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm plot\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TreeSHAP with Path Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP values\n",
    "explainer = shap.TreeExplainer(iso_forest)\n",
    "shap_values = explainer(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot for an anomaly\n",
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot for a normal instance\n",
    "shap.plots.waterfall(shap_values[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate f(x)\n",
    "path_length = shap_values.base_values + shap_values.values.sum(axis=1)\n",
    "\n",
    "# Get f(x) for anomalies and normal instances\n",
    "anomalies = data_aggregated[data_aggregated['anomaly'] == -1]\n",
    "path_length_anomalies = path_length[anomalies.index]\n",
    "\n",
    "normal = data_aggregated[data_aggregated['anomaly'] == 1]\n",
    "path_length_normal = path_length[normal.index]\n",
    "\n",
    "# Plot boxplots for f(x)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot([path_length_anomalies, path_length_normal], labels=['Anomaly','Normal'])\n",
    "plt.ylabel(\"Average Path Length f(x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeanSHAP\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeanSHAP\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction values\n",
    "shap_interaction_values = explainer.shap_interaction_values(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get absolute mean of matrices\n",
    "mean_shap = np.abs(shap_interaction_values).mean(0)\n",
    "mean_shap = np.round(mean_shap, 1)\n",
    "\n",
    "df = pd.DataFrame(mean_shap, index=features.columns, columns=features.columns)\n",
    "\n",
    "# Times off diagonal by 2\n",
    "df.where(df.values == np.diagonal(df), df.values * 2, inplace=True)\n",
    "\n",
    "# Display\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(df, cmap=\"coolwarm\", annot=True)\n",
    "plt.yticks(rotation=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shap",
   "language": "python",
   "name": "shap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
